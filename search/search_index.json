{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Inleiding \u00b6 Machine learning is het onderdeel van de informatica dat zich bezighoudt met het analyseren van grote hoeveelheden data en op basis daarvan structuren herkennen of voorspellingen doen. Hoewel de basis voor dit vakgebied al in de jaren zestig van de vorige eeuw is gelegd (bijvoorbeeld door het baanbrekende werk van Frank Rosenblatt , is het pas de voorbije twee decennia echt tot grote bloei gekomen. Dit heeft vooral te maken met de enorme hoeveelheid data die heden ten dage beschikbaar wordt gesteld (een fenomeen bekend onder de term information explosion in combinatie met de grote en goedkope computationele kracht van hedendaagse computers en data-centra. Opgaven \u00b6 Op deze pagina's zijn de weekopgaven van dit onderdeel te vinden. Deze opgaven worden gemaakt in duo's. Tijdens het eerste practicum wordt klassikaal een begin gemaakt met de opgaven: er wordt uitgelegd hoe te werken met numpy en hoe je lineaire algebra gebruikt in het domein van ML. Ook worden hier de weekopgaven verder toegelicht. Tijdens het tweede en derde practicum worden afspraken gemaakt met individuele duo's om het werk tot dan toe te bespreken, vragen te beantwoorden en opmerkingen te plaatsen. Tijdens deze gesprekken wordt ook het begrip van de materie getoetst. Er zijn drie weekopgaven die uiteindelijk in de voorlaatste week van het blok moeten zijn afgerond (je bent dus vrij om \u00e9\u00e9n en ander te plannen, zo lang je maar aan deze eis voldoet). Eventueel reparatiewerk wordt tijdens de gesprekken besproken. Mocht dat niet voor de deadline zijn afgerond, dan is er een uitloopmogelijkheid in de laatste week van het blok; dat geldt dan wel als een herkansing: als je hiervan gebruik maakt, kun je voor dit onderdeel maximaal een 6 halen. Voor het overige zijn de cijfers ONV, 6, 8 of 10. Opstarten \u00b6 De opgaven voor elke week gaan uit van een zipje met startcode. Het geheel gaat uit van een aantal dependencies. Deze dependencies hebben we voor het gemak in een requirements.txt gezet. Je kunt het beste een virtuele omgeving aanmaken en hierin met pip in \u00e9\u00e9n keer alle dependencies installeren.: baba @aurelia ~ % virtualenv ml created virtual environment CPython3 .8.7 . final .0 - 64 in 820 ms baba @aurelia ~ % cd ml baba @aurelia ml % cp ~/ Downloads / requirements . txt . baba @aurelia ml % source bin / activate ( ml ) baba @aurelia ml % python - m pip install - r requirements . txt ... ( ml ) baba @aurelia ml %","title":"Inleiding"},{"location":"index.html#inleiding","text":"Machine learning is het onderdeel van de informatica dat zich bezighoudt met het analyseren van grote hoeveelheden data en op basis daarvan structuren herkennen of voorspellingen doen. Hoewel de basis voor dit vakgebied al in de jaren zestig van de vorige eeuw is gelegd (bijvoorbeeld door het baanbrekende werk van Frank Rosenblatt , is het pas de voorbije twee decennia echt tot grote bloei gekomen. Dit heeft vooral te maken met de enorme hoeveelheid data die heden ten dage beschikbaar wordt gesteld (een fenomeen bekend onder de term information explosion in combinatie met de grote en goedkope computationele kracht van hedendaagse computers en data-centra.","title":"Inleiding"},{"location":"index.html#opgaven","text":"Op deze pagina's zijn de weekopgaven van dit onderdeel te vinden. Deze opgaven worden gemaakt in duo's. Tijdens het eerste practicum wordt klassikaal een begin gemaakt met de opgaven: er wordt uitgelegd hoe te werken met numpy en hoe je lineaire algebra gebruikt in het domein van ML. Ook worden hier de weekopgaven verder toegelicht. Tijdens het tweede en derde practicum worden afspraken gemaakt met individuele duo's om het werk tot dan toe te bespreken, vragen te beantwoorden en opmerkingen te plaatsen. Tijdens deze gesprekken wordt ook het begrip van de materie getoetst. Er zijn drie weekopgaven die uiteindelijk in de voorlaatste week van het blok moeten zijn afgerond (je bent dus vrij om \u00e9\u00e9n en ander te plannen, zo lang je maar aan deze eis voldoet). Eventueel reparatiewerk wordt tijdens de gesprekken besproken. Mocht dat niet voor de deadline zijn afgerond, dan is er een uitloopmogelijkheid in de laatste week van het blok; dat geldt dan wel als een herkansing: als je hiervan gebruik maakt, kun je voor dit onderdeel maximaal een 6 halen. Voor het overige zijn de cijfers ONV, 6, 8 of 10.","title":"Opgaven"},{"location":"index.html#opstarten","text":"De opgaven voor elke week gaan uit van een zipje met startcode. Het geheel gaat uit van een aantal dependencies. Deze dependencies hebben we voor het gemak in een requirements.txt gezet. Je kunt het beste een virtuele omgeving aanmaken en hierin met pip in \u00e9\u00e9n keer alle dependencies installeren.: baba @aurelia ~ % virtualenv ml created virtual environment CPython3 .8.7 . final .0 - 64 in 820 ms baba @aurelia ~ % cd ml baba @aurelia ml % cp ~/ Downloads / requirements . txt . baba @aurelia ml % source bin / activate ( ml ) baba @aurelia ml % python - m pip install - r requirements . txt ... ( ml ) baba @aurelia ml %","title":"Opstarten"},{"location":"week1.html","text":"Opgaven week 1 \u00b6 Inleiding \u00b6 Deze week werken we aan de lineaire regressie van data met \u00e9\u00e9n variabele. De data die we daarvoor gebruiken is de winst van een vervoerder gerelateerd aan de grootte van een stad waar die vervoerder werkzaam is. Je kunt je voorstellen dat het nuttig is om deze verhouding te weten, omdat je op basis hiervan ge\u00efnformeerd een besluit kunt nemen of je in een nieuwe stad (met een bepaalde grootte) een nieuw filiaal wilt openen. De startcode van deze opgave kun je hier downloaden . Het bestand week1_data.pkl bevat de data waar we mee gaan werken. Dit is een 97\u00d72-numpy-vector: de eerste kolom bevat de grootte van de steden, de tweede kolom bevat de winst van de vervoerder. Daarnaast vind je twee python-bestanden in de zip. Het bestand exercise1.py gebruikt de code in uitwerkingen.py om door de opgaven te lopen. Het is de bedoeling dat je het bestand uitwerkingen.py afmaakt. Bestudeer de code en het commentaar in beide bestanden om te zien hoe ze werken en wat de bedoeling ervan is. 1. het visualiseren van de data \u00b6 Een eerste stap in elk machine-learning project is een beeld cre\u00ebren van de data. Het eenvoudigst om dit te doen is door gebruik te maken van de library matplotlib . Hierin vind je een API pyplot waarmee je vrij eenvoudig een scatter plot kunt maken. Bestudeer de documentatie van pyplot en implementeer aan de hand hiervan de functie draw_graph in uitwerkingen.py , zodat je een afbeelding zoals de onderstaande krijgt. Hoewel de data niet echt normaal verdeeld is, is er wel een zekere verhouding waar te nemen tussen de grootte van de stad en de winst die de vervoerder maakt. In de rest van deze opgave gaan we deze verhouding bepalen. 2. Het bepalen van de cost function (de kostenfunctie) \u00b6 Zoals in de theorieles besproken is, is het bepalen van de verhouding tussen twee (of meer) grootheden een kwestie van het minimaliseren van de kostenfunctie: de som van de gekwadrateerde fouten (SSE) . Dit minimaliseren doe je door middel van gradient descent en het is dikwijls nuttig om die kostenfunctie gedurende de iteraties bij te houden, zodat je de verschillende uitkomsten door je data heen kunt plotten \u2013 zo kun je bijvoorbeeld controleren of je niet in een lokaal minimum terecht bent gekomen. In deze opgave programmeren we de kostenfunctie verder uit; de volgende opgave gaat verder in op de gradient descent. De kostenfunctie is gegeven door de volgende formule: \\[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} ( h_\\theta(x^{(i)}) - y^{(i)} ) ^2 \\] Hierin is \\(J(\\theta)\\) de totale kost die berekend wordt met de huidige waarden van \\(\\theta\\) . Verder is \\(h_{\\theta}(x)\\) de hypothese van de waarde (de voorspelling) en is \\(y\\) de actuele (daadwerkelijke) waarde. Door het verschil tussen deze twee waarden voor elk datapunt op te tellen en uiteindelijk uit te middelen, komen we op de voorspellende waarde die de formule heeft met de huidige waarden van \\(\\theta\\) . De algemene formule voor de hypothese (de voorspelling) is als volgt: \\[ h_\\theta(x) = \\theta^T\\vec{x} = \\theta_0 + \\theta_1x \\] Omdat we op zoek zijn naar een lijn, hebben we feitelijk te maken met \u00e9\u00e9n parameter (een lijn is immers \\(y=ax + b = b + ax\\) ). Allereerst bepalen we het aantal datapunten ( \\(m\\) ) en het aantal eigenschappen ( features , \\(n\\) ). Om de dimensionaliteit van de trainingsdata te laten corresponderen met \\(\\theta\\) , voegen we vervolgens een rij van enen toe. Daarna isoleren we de laatste kolom van de data om de gewenste waarden te krijgen (de vector \\(y\\) ); de rest van de data vormt dan de eigenlijke matrix \\(X\\) . Tenslotte initi\u00ebren we de vector \\(\\theta\\) : m , n = data . shape X = np . c_ [ np . ones ( m ), data [:, [ 0 ]]] y = data [:, [ 1 ]] theta = np . zeros ( ( 2 , 1 ) ) De voorspelde waarden in \\(X\\) , de actuele waarden in \\(y\\) en een \\(theta\\) worden aan de methode compute_cost meegegeven; deze functie moet de waarde van \\(J(\\theta)\\) teruggeven. Implementeer deze functie op basis van de beschrijving hierboven in het bestand uitwerkingen.py (zie ook de aanwijzingen in het bestand zelf); maak hem zo, dat hij werkt voor elke grootte van theta . Maak gebruik van een vectori\u00eble implementatie. Als je klaar bent, kun je het bestand opgaven.py runnen. Dit bestand roept `compute_cost aan en print de gevonden waarde uit. Als het goed is, krijg je uiteindelijk een waarde van rond de 32.07. 3a. Gradient descent \u00b6 Als laatste maak je de methode gradient_descent in het bestand uitwerkingen.py af. In deze methode wordt een aantal stappen uitgevoerd, waarbij in elke stap de vector \\(\\theta\\) volgens de onderstaande formule wordt aangepast. \\[ \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j \\] Als het goed is, zorgt elke stap in deze methode ervoor dat \\(J(\\theta)\\) lager wordt. Let er wel op dat je alle \\(\\theta_j\\) tegelijkertijd aanpast (in dit geval is de grootte van \\(\\theta\\) 2, dus elke iteratie moeten er twee parameters worden aangepast). Let er verder op dat je alleen \\(\\theta\\) aanpast: X en y zijn constante waarden die niet hoeven (of kunnen) te worden aangepast. De algemene structuur in het bestand is al gegeven. Als je implementatie klaar is, kun je opnieuw het bestand opgaven.py aanroepen; deze roept de functie gradient_descent aan, zodat de update 1500 keer wordt gedaan. Als het goed is, is \\(\\theta\\) uiteindelijk rond de (-3.63, 1.16). 3b. kosten bijhouden \u00b6 Omdat we, zoals gezegd, graag willen weten of de totale kost tijdens de gradient descent wel steeds minder wordt, is het van belang deze kosten tijdens deze descent bij te houden. Breid de functie gradient_descent aan zodat bij elke iteratie de functie compute_cost wordt aangeroepen met de huidige waarden van theta . Deze kosten stop je in de lijst costs (die al in de basiscode is ge\u00efnitialiseerd) en die retourneer je uiteindelijk eveneens ( gradient_descent retourneert dus twee waarden). Maak vervolgens de functie draw_costs in uitwerkingen.py . Deze functie moet de lijst meekrijgen die je hierboven in gradient_descent hebt gevuld. Maak gebruik van pyplot om deze lijst in een grafiekje te zetten. Als je dan het bestand exercise.py runt, krijg je als het goed is iets als het onderstaande plaatje: 4. Contour plot \u00b6 In deze laatste opgave gebruik je de methode compute_cost die je hierboven hebt gemaakt om een contour-plot van de kosten te tekenen. Hierdoor kun je inzicht krijgen in hoe deze waarde zich ontwikkelt bij verschillende waarden van \\(\\theta\\) . Het grootste deel van deze opgave is al in de methode contour_plot() in het bestand uitwerkingen.py gegeven; je hoeft alleen maar de waarden van de matrix J_val te vullen. Bestudeer het commentaar in het bestand voor meer toelichting. Als je klaar bent, roept het bestand exercise1.py de methode contour_plot() aan om de plot te tekenen. Als het goed is, ziet deze er ongeveer als hieronder uit.","title":"Opgaven week 1"},{"location":"week1.html#opgaven-week-1","text":"","title":"Opgaven week 1"},{"location":"week1.html#inleiding","text":"Deze week werken we aan de lineaire regressie van data met \u00e9\u00e9n variabele. De data die we daarvoor gebruiken is de winst van een vervoerder gerelateerd aan de grootte van een stad waar die vervoerder werkzaam is. Je kunt je voorstellen dat het nuttig is om deze verhouding te weten, omdat je op basis hiervan ge\u00efnformeerd een besluit kunt nemen of je in een nieuwe stad (met een bepaalde grootte) een nieuw filiaal wilt openen. De startcode van deze opgave kun je hier downloaden . Het bestand week1_data.pkl bevat de data waar we mee gaan werken. Dit is een 97\u00d72-numpy-vector: de eerste kolom bevat de grootte van de steden, de tweede kolom bevat de winst van de vervoerder. Daarnaast vind je twee python-bestanden in de zip. Het bestand exercise1.py gebruikt de code in uitwerkingen.py om door de opgaven te lopen. Het is de bedoeling dat je het bestand uitwerkingen.py afmaakt. Bestudeer de code en het commentaar in beide bestanden om te zien hoe ze werken en wat de bedoeling ervan is.","title":"Inleiding"},{"location":"week1.html#1-het-visualiseren-van-de-data","text":"Een eerste stap in elk machine-learning project is een beeld cre\u00ebren van de data. Het eenvoudigst om dit te doen is door gebruik te maken van de library matplotlib . Hierin vind je een API pyplot waarmee je vrij eenvoudig een scatter plot kunt maken. Bestudeer de documentatie van pyplot en implementeer aan de hand hiervan de functie draw_graph in uitwerkingen.py , zodat je een afbeelding zoals de onderstaande krijgt. Hoewel de data niet echt normaal verdeeld is, is er wel een zekere verhouding waar te nemen tussen de grootte van de stad en de winst die de vervoerder maakt. In de rest van deze opgave gaan we deze verhouding bepalen.","title":"1. het visualiseren van de data"},{"location":"week1.html#2-het-bepalen-van-de-cost-function-de-kostenfunctie","text":"Zoals in de theorieles besproken is, is het bepalen van de verhouding tussen twee (of meer) grootheden een kwestie van het minimaliseren van de kostenfunctie: de som van de gekwadrateerde fouten (SSE) . Dit minimaliseren doe je door middel van gradient descent en het is dikwijls nuttig om die kostenfunctie gedurende de iteraties bij te houden, zodat je de verschillende uitkomsten door je data heen kunt plotten \u2013 zo kun je bijvoorbeeld controleren of je niet in een lokaal minimum terecht bent gekomen. In deze opgave programmeren we de kostenfunctie verder uit; de volgende opgave gaat verder in op de gradient descent. De kostenfunctie is gegeven door de volgende formule: \\[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} ( h_\\theta(x^{(i)}) - y^{(i)} ) ^2 \\] Hierin is \\(J(\\theta)\\) de totale kost die berekend wordt met de huidige waarden van \\(\\theta\\) . Verder is \\(h_{\\theta}(x)\\) de hypothese van de waarde (de voorspelling) en is \\(y\\) de actuele (daadwerkelijke) waarde. Door het verschil tussen deze twee waarden voor elk datapunt op te tellen en uiteindelijk uit te middelen, komen we op de voorspellende waarde die de formule heeft met de huidige waarden van \\(\\theta\\) . De algemene formule voor de hypothese (de voorspelling) is als volgt: \\[ h_\\theta(x) = \\theta^T\\vec{x} = \\theta_0 + \\theta_1x \\] Omdat we op zoek zijn naar een lijn, hebben we feitelijk te maken met \u00e9\u00e9n parameter (een lijn is immers \\(y=ax + b = b + ax\\) ). Allereerst bepalen we het aantal datapunten ( \\(m\\) ) en het aantal eigenschappen ( features , \\(n\\) ). Om de dimensionaliteit van de trainingsdata te laten corresponderen met \\(\\theta\\) , voegen we vervolgens een rij van enen toe. Daarna isoleren we de laatste kolom van de data om de gewenste waarden te krijgen (de vector \\(y\\) ); de rest van de data vormt dan de eigenlijke matrix \\(X\\) . Tenslotte initi\u00ebren we de vector \\(\\theta\\) : m , n = data . shape X = np . c_ [ np . ones ( m ), data [:, [ 0 ]]] y = data [:, [ 1 ]] theta = np . zeros ( ( 2 , 1 ) ) De voorspelde waarden in \\(X\\) , de actuele waarden in \\(y\\) en een \\(theta\\) worden aan de methode compute_cost meegegeven; deze functie moet de waarde van \\(J(\\theta)\\) teruggeven. Implementeer deze functie op basis van de beschrijving hierboven in het bestand uitwerkingen.py (zie ook de aanwijzingen in het bestand zelf); maak hem zo, dat hij werkt voor elke grootte van theta . Maak gebruik van een vectori\u00eble implementatie. Als je klaar bent, kun je het bestand opgaven.py runnen. Dit bestand roept `compute_cost aan en print de gevonden waarde uit. Als het goed is, krijg je uiteindelijk een waarde van rond de 32.07.","title":"2. Het bepalen van de cost function (de kostenfunctie)"},{"location":"week1.html#3a-gradient-descent","text":"Als laatste maak je de methode gradient_descent in het bestand uitwerkingen.py af. In deze methode wordt een aantal stappen uitgevoerd, waarbij in elke stap de vector \\(\\theta\\) volgens de onderstaande formule wordt aangepast. \\[ \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j \\] Als het goed is, zorgt elke stap in deze methode ervoor dat \\(J(\\theta)\\) lager wordt. Let er wel op dat je alle \\(\\theta_j\\) tegelijkertijd aanpast (in dit geval is de grootte van \\(\\theta\\) 2, dus elke iteratie moeten er twee parameters worden aangepast). Let er verder op dat je alleen \\(\\theta\\) aanpast: X en y zijn constante waarden die niet hoeven (of kunnen) te worden aangepast. De algemene structuur in het bestand is al gegeven. Als je implementatie klaar is, kun je opnieuw het bestand opgaven.py aanroepen; deze roept de functie gradient_descent aan, zodat de update 1500 keer wordt gedaan. Als het goed is, is \\(\\theta\\) uiteindelijk rond de (-3.63, 1.16).","title":"3a. Gradient descent"},{"location":"week1.html#3b-kosten-bijhouden","text":"Omdat we, zoals gezegd, graag willen weten of de totale kost tijdens de gradient descent wel steeds minder wordt, is het van belang deze kosten tijdens deze descent bij te houden. Breid de functie gradient_descent aan zodat bij elke iteratie de functie compute_cost wordt aangeroepen met de huidige waarden van theta . Deze kosten stop je in de lijst costs (die al in de basiscode is ge\u00efnitialiseerd) en die retourneer je uiteindelijk eveneens ( gradient_descent retourneert dus twee waarden). Maak vervolgens de functie draw_costs in uitwerkingen.py . Deze functie moet de lijst meekrijgen die je hierboven in gradient_descent hebt gevuld. Maak gebruik van pyplot om deze lijst in een grafiekje te zetten. Als je dan het bestand exercise.py runt, krijg je als het goed is iets als het onderstaande plaatje:","title":"3b. kosten bijhouden"},{"location":"week1.html#4-contour-plot","text":"In deze laatste opgave gebruik je de methode compute_cost die je hierboven hebt gemaakt om een contour-plot van de kosten te tekenen. Hierdoor kun je inzicht krijgen in hoe deze waarde zich ontwikkelt bij verschillende waarden van \\(\\theta\\) . Het grootste deel van deze opgave is al in de methode contour_plot() in het bestand uitwerkingen.py gegeven; je hoeft alleen maar de waarden van de matrix J_val te vullen. Bestudeer het commentaar in het bestand voor meer toelichting. Als je klaar bent, roept het bestand exercise1.py de methode contour_plot() aan om de plot te tekenen. Als het goed is, ziet deze er ongeveer als hieronder uit.","title":"4. Contour plot"},{"location":"week2.html","text":"Opgaven week 2 \u00b6 Inleiding \u00b6 Deze en de volgende week staan in het teken van standaard datasets. Deze week werken met de zogenaamde MNIST dataset : een set van zevenduizend gray scale afbeeldingen van cijfers en letters geschreven door middelbare scholieren. Werken met de MNIST dataset is te vergelijken met de hello world van machine learning : vroeg of laat krijg je ermee te maken. Deze week programmeren we zelf een neuraal netwerk aan de hand van reeds geleerde gewichten; in de laatste week zullen we een framework gebruiken in een poging een andere dataset te classificeren. De set die we in deze week gebruiken is een subset van de oorspronkelijke dataset. Het gaat om vijfduizend samples, waarbij elk sample een plaatje van 20 bij 20 pixels is dat een getal van 0 tot 9 representeert. Elke kolom van deze 20 \u00d7 20 matrix is onder de vorige geplakt, zodat er uiteindelijke een 400 \u00d71 vector ontstaat. Deze vectoren zijn weer getransponeerd, zodat onze dataset \\(X\\) uiteindelijk een 5000 \u00d7 400 matrix is. \\[ X = \\begin{bmatrix} & \u2014 & (x^{(1)})^T & \u2014 & \\\\ & \u2014 & (x^{(2)})^T & \u2014 & \\\\ & & \\vdots & & \\\\ & \u2014 & (x^{(m)})^T & \u2014 & \\\\ \\end{bmatrix} \\] In deze matrix is $x^{(1)}$ de vector van het eerste plaatje, $x^{(2)}$ de vector van het tweede plaatje, enzovoort. Behalve deze X-matrix is er in de data ook een 5000\u00d71 vector y gegeven, waarin per plaatje is aangegeven welk cijfer dit representeert. Om problemen met de 0 te voorkomen, is in deze vector 0 weergegeven als 10. De startcode van deze week is hier te downloaden . Opnieuw wordt deze opgave doorlopen door het script exercise2.py . Dit script importeert de functies uit uitwerkingen.py en runt die op volgorde. Het is de opgave om deze uitwerkingen af te maken. Bestudeer beide scripts om een idee te krijgen van de werking. 1. het visualiseren van de data \u00b6 Zoals altijd gaan we eerst de data visualiseren. In dit geval betekent dat een vector uit \\(x\\) weer transformeren in een 20\u00d720 gray scale plaatje. Maak hiervoor de functie plotNumber() af. Omdat je weet dat de vector \\(x^{(i)}\\) het i-de plaatje uit de dataset representeert, kun je deze vector eenvoudig weer terug omzetten in een 20\u00d720 matrix en die tekenen. Maak daarbij gebruik van de methode numpy.reshape en van matplotlib.matshow . Het script roept de methode plotNumber aan met een willekeurige vector uit de matrix X. Het toont het nummer op de console, en het cijfer dat het plaatje zou moeten voorstellen. Op die manier kun je eenvoudig checken of je uitwerking correct is. Als je deze opgave hebt afgerond, kun je het script aanroepen met skip als command line parameter: het tekenen wordt dan overgeslagen. 2. het neurale netwerk - forward propagation \u00b6 Het neurale netwerk dat we voor deze week gaan uitprogrammeren bestaat uit drie lagen. De input van het netwerk zijn de 20\u00d720 plaatjes van de handgeschreven cijfers, dus de input-laag bestaat uit 400 nodes. De middelste verborgen laag heeft 25 nodes en de output-laag heeft tien nodes \u2013 \u00e9\u00e9n voor elk cijfer van nul tot en met negen. Gegeven een bepaalde input moet \u00e9\u00e9n van de tien output nodes de hoogste waarde hebben. 2a: de sigmoid functie \u00b6 Het bepalen van het cijfer dat door het plaatje wordt gerepresenteerd is feitelijk een classificatie probleem: de input vector \\(x^{(i)}\\) moet immers geclassificeerd worden als \u00e9\u00e9n van de cijfers 0 tot en met 9. Zoals tijdens de theorieles is besproken, hebben we voor dergelijke problemen de sigmoid functie \\(g(z)\\) nodig. De formule daarvan is als volgt: \\[ g(z) = \\frac{1}{1+e^{-z}} \\] Implementeer de methode sigmoid in uitwerkingen.py . Maak hem zo, dat je er zowel een getal als een vector aan kunt meegeven. In het eerste geval moet de functie de sigmoid-waarde van het getal retourneren, in het tweede geval moet hij een vector retourneren met de sigmoid-waarde van elk individueel element in de input-vector. Je kunt gebruik maken van de numpy-functie exp() . Als je klaar bent, kun je het script exercise2.py runnen. Deze roept de methode vijf keer aan: drie keer met individuele getallen -10, 0, en 10; vervolgens met deze drie getallen als een kolomvector en als een rijvector. 2b: omzetten van een vector naar een matrix \u00b6 We beginnen met het omzetten van de rijvector \\(y\\) naar een matrix. Voor het berekenen van de kost van het netwerk moeten we namelijk deze vector omzetten in een 5000\u00d710 matrix van enen en nullen, waarbij het i-de element 1 is en de rest 0. Als bijvoorbeeld het label in \\(y\\) gelijk is aan 5, dan moeten we deze rij omzetten in een 10-dimensionale vector met een 1 op positie 5 en een 0 op de rest. Omdat de 0 in de y-vector gerepresenteerd wordt als 10, moet deze in de resulterende matrix een 1 krijgen op de eerste positie (met index 0). \\[ y=\\begin{bmatrix}1\\\\0\\\\0\\\\\\vdots\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\0\\\\\\vdots\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\0\\\\1\\\\\\vdots\\\\0\\end{bmatrix}, ... , of \\begin{bmatrix}0\\\\0\\\\0\\\\\\vdots\\\\1\\end{bmatrix} \\] Maak de methode get_y_matrix() in uitwerking.py af. Hierbij kun je gebruik maken van de methode csr_matrix uit scipy om op basis van y de matrix y_vec te maken. Let er daarbij wel op dat de y-vector 1-based is, terwijl de resulterende matrix 0-based moet zijn. Zie het onderstaande code-fragment voor een voorbeeld (je kunt ook gebruik maken van de methode todense() om een zogenaamde ijle matrix te maken): >>> import numpy as np >>> from scipy.sparse import csr_matrix >>> cols = np . array ([ 2 , 1 , 3 , 5 ]) >>> rows = [ i for i in range ( len ( cols ))] >>> data = [ 1 for _ in range ( len ( cols ))] >>> width = max ( cols ) + 1 # arrays zijn zero-based >>> y_vec = csr_matrix (( data , ( rows , cols )), shape = ( len ( rows ), width + 1 )) . toarray () >>> y_vec array ([[ 0 , 0 , 1 , 0 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 1 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 1 , 0 ]]) >>> Als je klaar bent, kun je het script exercise2.py opnieuw runnen om de geretourneerde waarde te controleren. 2c. voorspel het getal en bepaal de kost van deze voorspelling. \u00b6 Implementeer nu de methode predict_number . Maak hierbij gebruik van het stappenplan dat is gegeven in de afbeelding van het netwerk hierboven, en van het commentaar in de opgave. Let er bij je implementatie op dat de matrix X de waarden voor de input-nodes als rij bevat (400 rijen), terwijl de matrices \\(\\Theta^{(1)}\\) en \\(\\Theta^{(2)}\\) de waarde voor elke node als kolom bevatten (zo is \\(\\Theta^{(1)}_{2,3}\\) het gewicht tussen de derde input-node en de tweede verborgen node). Voorzie de data van de juiste indices en transpositioneer de matrix waar dat nodig is. De output van deze methode is een 10 &times 1 vector die voor elk getal de waarschijnlijkheid dat de input dat getal is weergeeft. Om de accuratesse van deze voorspelling te bepalen, moeten we deze voorspelling vergelijken met de betreffende regel uit de y_matrix die we in het eerste deel van deze opgave hebben gemaakt. Dat gebeurt in de methode compute_cost() . De formule voor de kost is als volgt: \\[ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}\\left[-y_k^{(i)}log((h_\\theta(x^{(i)}))_k) - (1-y_k^{(i)})log(1-(h_\\theta(x^{(i)}))_k\\right] \\] Als je deze drie methoden hebt ge\u00efmplementeerd, kun je het script exercise2.py opnieuw aanroepen. Hier wordt begonnen met min of meer willekeurige waarden van de Theta's. We zetten deze niet op 0 (waarom niet?), maar verdelen we uniform in de range \\([-0.12, 0.12]\\) \u2013 zie hiervoor de methode initialize_random_weights in het script. Deze waarde is gebaseerd op het aantal nodes in het netwerk en wordt benaderd door \\[ \\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in}+L{out}}} \\] waarbij \\(L_{in}\\) en \\(L_{out}\\) staan voor het aantal nodes links en rechts van de betreffende laag. Het script voert met deze waarden van de matrices een forward propagation stap uit en toont de kost die correspondeert met de huidige waarden van de beide Theta 's. Als het goed is, ligt dit zo rond de 7 (de exact waarde is natuurlijk moeilijk te voorspellen). Vervolgens wordt de huidige accuratesse van het netwerk getoond (die is vanzelfsprekend extreem laag). 3. het neurale netwerk \u2013 backpropagation \u00b6 3a. de sigmo\u00efdegradi\u00ebnt \u00b6 Om de relatieve bijdrage van een node te bepalen, hebben we de afgeleide van de sigmo\u00efdefunctie nodig. Deze is hieronder gegeven. \\[ g'(z) = \\frac{d}{dz}g(z) = g(z)(1-g(z)) \\] Implementeer deze afgeleide in de methode sigmoid_gradient . Zorg er opnieuw (net als bij de sigmo\u00efde zelf) voor dat deze methode zowel met scalaire waarden als met vectoren kan werken. Om zeker te weten of het goed is gegaan, roept het script exercise.py deze methode weer aan met drie verschillende waarden. 3b. backpropagation \u00b6 e gaan nu de backpropagation uitprogrammeren in de methode nn_check_gradients . De intu\u00eftie hierbij is als volgt: voor een observatie \\((x^{(i)}, y^{(i)})\\) doen we eerst een forward-propagation stap, waarbij we de voorspelling \\(a_3 = h_\\Theta(x^{(i)})\\) uitrekenen. Als we die hebben bepaald, willen we voor elke node in elke laag in het netwerk bepalen wat de bijdrage van deze node aan de totale fout is geweest. Deze fout geven we weer met \\(\\delta_j^{(l)}\\) , waarbij \\(l\\) de laag en \\(j\\) het nummer van de betreffende node is. Voor de output-laag is de fout redelijk rechttoe-rechtaan te bepalen: dit is per node het verschil tussen diens output en de gewenste output (zoals opgeslagen in de matrix y_vec ). Voor de verborgen nodes is dit iets complexer: de fout in laag \\(l\\) wordt berekend aan de hand van de gemiddelde fout in de laag \\(l+1\\) . Zie het stappenplan hieronder; we adviseren je om de backpropagation uit te programmeren in een for-lus ( for i in range (m): ), omdat een vectori\u00eble implementatie complex is en een na\u00efeve implementatie waarschijnlijk leerzamer. Stap 1 Gegeven een input \\(a^{(i)}\\) , doe een standaard forward-propagation door gebruik te maken van de code uit predict_number ; je moet de code hier wel herhalen, omdat je de verschillende waarden nodig hebt tijdens de backpropagation. Stap 2 Zet voor elke output-node k in de derde laag \\[ \\delta_k^{(3)} = (a_k^{(3)} - y_k) \\] Stap 3 Voor elke node in de verborgen (tweede) laag bepaal je diens 'bijdrage' aan de totale fout door het inproduct van de fout en de matrix ( element wise ) te vermenigvuldigen met de afgeleide van de sigmo\u00efdefunctie (die we in het eerste deel van deze opgave hebben gemaakt). \\[ \\delta^{(2)} = (\\Theta^{(2)})^T\\cdot\\delta^{(3)} \\times g'(z^{(2)}) \\] Stap 4 Deze bijdrage tellen we op bij de andere bijdragen; op deze manier cre\u00ebren we twee nieuwe matrices \\(\\Delta^{(1)}\\) en \\(\\Delta^{(2)}\\) , die dezelfde dimensionaliteit hebben als \\(\\Theta^{(1)}\\) en \\(\\Theta^{(2)}\\) . \\[ \\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}\\cdot(a^{(l)})^T \\] Stap 5 Als we deze stappen voor alle \\(m\\) observaties hebben gedaan, delen we de beide delta-matrices door het aantal observaties ( element wise ) om de gemiddelde fout per node voor de huidige waarden van de beide theta-matrices te verkrijgen. Retourneer deze beide waarden uit de methode nn_check_gradients . Wanneer je dit hebt gedaan, kun je het script verder laten runnen. Nu wordt van beide matrices de totale som afgedruk; die is nog behoorlijk hoog en het is de bedoeling dat we deze som, en de totale kosten, naar beneden brengen. Daar gaat de laatste opgave over. 4. trainen van het neurale netwerk \u00b6 Nu het netwerk goed is ge\u00efmplementeerd, kunnen we het gaan trainen. Het idee van die training is dat de waarden in de matrices langzaamaan naar een optimale waarde convergeren, waarmee de voorspelling voor een bepaalde input correspondeert met een goede output (een goede classificatie). Om dit gedaan te krijgen, maken we gebruik van de methode minimize uit scipy.optimize . Deze methode heeft een aantal parameters, waarvan \u00e9\u00e9n het maximaal aantal iteraties is. De methode stopt wanneer het maximaal aantal iteraties is behaald, of het minimale waarde van de kosten heeft gevonden (wat maar het eerst optreedt). Initieel staat deze parameter op 30. Let op: het trainen van het netwerk kan even duren (je krijgt wel het iteratienummer te zien). Je hoeft voor deze opgave niets uit te programmeren. Bestudeer de werking van minimize en experimenteer met verschillende waarden voor de parameter maxiter om een goed beeld te krijgen van de werking van het geheel. Bekijk ook goed hoe de waarden van de matrices worden doorgegeven aan deze methode. Als het geheel is afgerond, worden de nieuwe kost en de nieuwe accuratesse van het netwerk getoond. Ook wordt er een plot gemaakt van de waarden van de matrix in de verborgen laag \u2013 zie de figuur hieronder. Met een beetje moeite en goede wil kun je zien dat deze matix een gevoeligheid heeft ontwikkeld voor horizontale en verticale lijnen en voor ronde vormen in de input. Met de getrainde waarde van het netwerk kunnen we nu voorspellingen doen over het getal dat door een afbeelding wordt gerepresenteerd. Maar de accuratesse is niet de enige metriek die voor de bepaling van hoe goed een netwerk is van belang is. Hierover gaan we het volgende week hebben, wanneer we de confusion matrix bespreken.","title":"Opgaven week 2"},{"location":"week2.html#opgaven-week-2","text":"","title":"Opgaven week 2"},{"location":"week2.html#inleiding","text":"Deze en de volgende week staan in het teken van standaard datasets. Deze week werken met de zogenaamde MNIST dataset : een set van zevenduizend gray scale afbeeldingen van cijfers en letters geschreven door middelbare scholieren. Werken met de MNIST dataset is te vergelijken met de hello world van machine learning : vroeg of laat krijg je ermee te maken. Deze week programmeren we zelf een neuraal netwerk aan de hand van reeds geleerde gewichten; in de laatste week zullen we een framework gebruiken in een poging een andere dataset te classificeren. De set die we in deze week gebruiken is een subset van de oorspronkelijke dataset. Het gaat om vijfduizend samples, waarbij elk sample een plaatje van 20 bij 20 pixels is dat een getal van 0 tot 9 representeert. Elke kolom van deze 20 \u00d7 20 matrix is onder de vorige geplakt, zodat er uiteindelijke een 400 \u00d71 vector ontstaat. Deze vectoren zijn weer getransponeerd, zodat onze dataset \\(X\\) uiteindelijk een 5000 \u00d7 400 matrix is. \\[ X = \\begin{bmatrix} & \u2014 & (x^{(1)})^T & \u2014 & \\\\ & \u2014 & (x^{(2)})^T & \u2014 & \\\\ & & \\vdots & & \\\\ & \u2014 & (x^{(m)})^T & \u2014 & \\\\ \\end{bmatrix} \\] In deze matrix is $x^{(1)}$ de vector van het eerste plaatje, $x^{(2)}$ de vector van het tweede plaatje, enzovoort. Behalve deze X-matrix is er in de data ook een 5000\u00d71 vector y gegeven, waarin per plaatje is aangegeven welk cijfer dit representeert. Om problemen met de 0 te voorkomen, is in deze vector 0 weergegeven als 10. De startcode van deze week is hier te downloaden . Opnieuw wordt deze opgave doorlopen door het script exercise2.py . Dit script importeert de functies uit uitwerkingen.py en runt die op volgorde. Het is de opgave om deze uitwerkingen af te maken. Bestudeer beide scripts om een idee te krijgen van de werking.","title":"Inleiding"},{"location":"week2.html#1-het-visualiseren-van-de-data","text":"Zoals altijd gaan we eerst de data visualiseren. In dit geval betekent dat een vector uit \\(x\\) weer transformeren in een 20\u00d720 gray scale plaatje. Maak hiervoor de functie plotNumber() af. Omdat je weet dat de vector \\(x^{(i)}\\) het i-de plaatje uit de dataset representeert, kun je deze vector eenvoudig weer terug omzetten in een 20\u00d720 matrix en die tekenen. Maak daarbij gebruik van de methode numpy.reshape en van matplotlib.matshow . Het script roept de methode plotNumber aan met een willekeurige vector uit de matrix X. Het toont het nummer op de console, en het cijfer dat het plaatje zou moeten voorstellen. Op die manier kun je eenvoudig checken of je uitwerking correct is. Als je deze opgave hebt afgerond, kun je het script aanroepen met skip als command line parameter: het tekenen wordt dan overgeslagen.","title":"1. het visualiseren van de data"},{"location":"week2.html#2-het-neurale-netwerk-forward-propagation","text":"Het neurale netwerk dat we voor deze week gaan uitprogrammeren bestaat uit drie lagen. De input van het netwerk zijn de 20\u00d720 plaatjes van de handgeschreven cijfers, dus de input-laag bestaat uit 400 nodes. De middelste verborgen laag heeft 25 nodes en de output-laag heeft tien nodes \u2013 \u00e9\u00e9n voor elk cijfer van nul tot en met negen. Gegeven een bepaalde input moet \u00e9\u00e9n van de tien output nodes de hoogste waarde hebben.","title":"2. het neurale netwerk - forward propagation"},{"location":"week2.html#2a-de-sigmoid-functie","text":"Het bepalen van het cijfer dat door het plaatje wordt gerepresenteerd is feitelijk een classificatie probleem: de input vector \\(x^{(i)}\\) moet immers geclassificeerd worden als \u00e9\u00e9n van de cijfers 0 tot en met 9. Zoals tijdens de theorieles is besproken, hebben we voor dergelijke problemen de sigmoid functie \\(g(z)\\) nodig. De formule daarvan is als volgt: \\[ g(z) = \\frac{1}{1+e^{-z}} \\] Implementeer de methode sigmoid in uitwerkingen.py . Maak hem zo, dat je er zowel een getal als een vector aan kunt meegeven. In het eerste geval moet de functie de sigmoid-waarde van het getal retourneren, in het tweede geval moet hij een vector retourneren met de sigmoid-waarde van elk individueel element in de input-vector. Je kunt gebruik maken van de numpy-functie exp() . Als je klaar bent, kun je het script exercise2.py runnen. Deze roept de methode vijf keer aan: drie keer met individuele getallen -10, 0, en 10; vervolgens met deze drie getallen als een kolomvector en als een rijvector.","title":"2a: de sigmoid functie"},{"location":"week2.html#2b-omzetten-van-een-vector-naar-een-matrix","text":"We beginnen met het omzetten van de rijvector \\(y\\) naar een matrix. Voor het berekenen van de kost van het netwerk moeten we namelijk deze vector omzetten in een 5000\u00d710 matrix van enen en nullen, waarbij het i-de element 1 is en de rest 0. Als bijvoorbeeld het label in \\(y\\) gelijk is aan 5, dan moeten we deze rij omzetten in een 10-dimensionale vector met een 1 op positie 5 en een 0 op de rest. Omdat de 0 in de y-vector gerepresenteerd wordt als 10, moet deze in de resulterende matrix een 1 krijgen op de eerste positie (met index 0). \\[ y=\\begin{bmatrix}1\\\\0\\\\0\\\\\\vdots\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\0\\\\\\vdots\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\0\\\\1\\\\\\vdots\\\\0\\end{bmatrix}, ... , of \\begin{bmatrix}0\\\\0\\\\0\\\\\\vdots\\\\1\\end{bmatrix} \\] Maak de methode get_y_matrix() in uitwerking.py af. Hierbij kun je gebruik maken van de methode csr_matrix uit scipy om op basis van y de matrix y_vec te maken. Let er daarbij wel op dat de y-vector 1-based is, terwijl de resulterende matrix 0-based moet zijn. Zie het onderstaande code-fragment voor een voorbeeld (je kunt ook gebruik maken van de methode todense() om een zogenaamde ijle matrix te maken): >>> import numpy as np >>> from scipy.sparse import csr_matrix >>> cols = np . array ([ 2 , 1 , 3 , 5 ]) >>> rows = [ i for i in range ( len ( cols ))] >>> data = [ 1 for _ in range ( len ( cols ))] >>> width = max ( cols ) + 1 # arrays zijn zero-based >>> y_vec = csr_matrix (( data , ( rows , cols )), shape = ( len ( rows ), width + 1 )) . toarray () >>> y_vec array ([[ 0 , 0 , 1 , 0 , 0 , 0 , 0 ], [ 0 , 1 , 0 , 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 1 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 , 0 , 1 , 0 ]]) >>> Als je klaar bent, kun je het script exercise2.py opnieuw runnen om de geretourneerde waarde te controleren.","title":"2b: omzetten van een vector naar een matrix"},{"location":"week2.html#2c-voorspel-het-getal-en-bepaal-de-kost-van-deze-voorspelling","text":"Implementeer nu de methode predict_number . Maak hierbij gebruik van het stappenplan dat is gegeven in de afbeelding van het netwerk hierboven, en van het commentaar in de opgave. Let er bij je implementatie op dat de matrix X de waarden voor de input-nodes als rij bevat (400 rijen), terwijl de matrices \\(\\Theta^{(1)}\\) en \\(\\Theta^{(2)}\\) de waarde voor elke node als kolom bevatten (zo is \\(\\Theta^{(1)}_{2,3}\\) het gewicht tussen de derde input-node en de tweede verborgen node). Voorzie de data van de juiste indices en transpositioneer de matrix waar dat nodig is. De output van deze methode is een 10 &times 1 vector die voor elk getal de waarschijnlijkheid dat de input dat getal is weergeeft. Om de accuratesse van deze voorspelling te bepalen, moeten we deze voorspelling vergelijken met de betreffende regel uit de y_matrix die we in het eerste deel van deze opgave hebben gemaakt. Dat gebeurt in de methode compute_cost() . De formule voor de kost is als volgt: \\[ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}\\left[-y_k^{(i)}log((h_\\theta(x^{(i)}))_k) - (1-y_k^{(i)})log(1-(h_\\theta(x^{(i)}))_k\\right] \\] Als je deze drie methoden hebt ge\u00efmplementeerd, kun je het script exercise2.py opnieuw aanroepen. Hier wordt begonnen met min of meer willekeurige waarden van de Theta's. We zetten deze niet op 0 (waarom niet?), maar verdelen we uniform in de range \\([-0.12, 0.12]\\) \u2013 zie hiervoor de methode initialize_random_weights in het script. Deze waarde is gebaseerd op het aantal nodes in het netwerk en wordt benaderd door \\[ \\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in}+L{out}}} \\] waarbij \\(L_{in}\\) en \\(L_{out}\\) staan voor het aantal nodes links en rechts van de betreffende laag. Het script voert met deze waarden van de matrices een forward propagation stap uit en toont de kost die correspondeert met de huidige waarden van de beide Theta 's. Als het goed is, ligt dit zo rond de 7 (de exact waarde is natuurlijk moeilijk te voorspellen). Vervolgens wordt de huidige accuratesse van het netwerk getoond (die is vanzelfsprekend extreem laag).","title":"2c. voorspel het getal en bepaal de kost van deze voorspelling."},{"location":"week2.html#3-het-neurale-netwerk-backpropagation","text":"","title":"3. het neurale netwerk \u2013 backpropagation"},{"location":"week2.html#3a-de-sigmoidegradient","text":"Om de relatieve bijdrage van een node te bepalen, hebben we de afgeleide van de sigmo\u00efdefunctie nodig. Deze is hieronder gegeven. \\[ g'(z) = \\frac{d}{dz}g(z) = g(z)(1-g(z)) \\] Implementeer deze afgeleide in de methode sigmoid_gradient . Zorg er opnieuw (net als bij de sigmo\u00efde zelf) voor dat deze methode zowel met scalaire waarden als met vectoren kan werken. Om zeker te weten of het goed is gegaan, roept het script exercise.py deze methode weer aan met drie verschillende waarden.","title":"3a. de sigmo\u00efdegradi\u00ebnt"},{"location":"week2.html#3b-backpropagation","text":"e gaan nu de backpropagation uitprogrammeren in de methode nn_check_gradients . De intu\u00eftie hierbij is als volgt: voor een observatie \\((x^{(i)}, y^{(i)})\\) doen we eerst een forward-propagation stap, waarbij we de voorspelling \\(a_3 = h_\\Theta(x^{(i)})\\) uitrekenen. Als we die hebben bepaald, willen we voor elke node in elke laag in het netwerk bepalen wat de bijdrage van deze node aan de totale fout is geweest. Deze fout geven we weer met \\(\\delta_j^{(l)}\\) , waarbij \\(l\\) de laag en \\(j\\) het nummer van de betreffende node is. Voor de output-laag is de fout redelijk rechttoe-rechtaan te bepalen: dit is per node het verschil tussen diens output en de gewenste output (zoals opgeslagen in de matrix y_vec ). Voor de verborgen nodes is dit iets complexer: de fout in laag \\(l\\) wordt berekend aan de hand van de gemiddelde fout in de laag \\(l+1\\) . Zie het stappenplan hieronder; we adviseren je om de backpropagation uit te programmeren in een for-lus ( for i in range (m): ), omdat een vectori\u00eble implementatie complex is en een na\u00efeve implementatie waarschijnlijk leerzamer. Stap 1 Gegeven een input \\(a^{(i)}\\) , doe een standaard forward-propagation door gebruik te maken van de code uit predict_number ; je moet de code hier wel herhalen, omdat je de verschillende waarden nodig hebt tijdens de backpropagation. Stap 2 Zet voor elke output-node k in de derde laag \\[ \\delta_k^{(3)} = (a_k^{(3)} - y_k) \\] Stap 3 Voor elke node in de verborgen (tweede) laag bepaal je diens 'bijdrage' aan de totale fout door het inproduct van de fout en de matrix ( element wise ) te vermenigvuldigen met de afgeleide van de sigmo\u00efdefunctie (die we in het eerste deel van deze opgave hebben gemaakt). \\[ \\delta^{(2)} = (\\Theta^{(2)})^T\\cdot\\delta^{(3)} \\times g'(z^{(2)}) \\] Stap 4 Deze bijdrage tellen we op bij de andere bijdragen; op deze manier cre\u00ebren we twee nieuwe matrices \\(\\Delta^{(1)}\\) en \\(\\Delta^{(2)}\\) , die dezelfde dimensionaliteit hebben als \\(\\Theta^{(1)}\\) en \\(\\Theta^{(2)}\\) . \\[ \\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}\\cdot(a^{(l)})^T \\] Stap 5 Als we deze stappen voor alle \\(m\\) observaties hebben gedaan, delen we de beide delta-matrices door het aantal observaties ( element wise ) om de gemiddelde fout per node voor de huidige waarden van de beide theta-matrices te verkrijgen. Retourneer deze beide waarden uit de methode nn_check_gradients . Wanneer je dit hebt gedaan, kun je het script verder laten runnen. Nu wordt van beide matrices de totale som afgedruk; die is nog behoorlijk hoog en het is de bedoeling dat we deze som, en de totale kosten, naar beneden brengen. Daar gaat de laatste opgave over.","title":"3b. backpropagation"},{"location":"week2.html#4-trainen-van-het-neurale-netwerk","text":"Nu het netwerk goed is ge\u00efmplementeerd, kunnen we het gaan trainen. Het idee van die training is dat de waarden in de matrices langzaamaan naar een optimale waarde convergeren, waarmee de voorspelling voor een bepaalde input correspondeert met een goede output (een goede classificatie). Om dit gedaan te krijgen, maken we gebruik van de methode minimize uit scipy.optimize . Deze methode heeft een aantal parameters, waarvan \u00e9\u00e9n het maximaal aantal iteraties is. De methode stopt wanneer het maximaal aantal iteraties is behaald, of het minimale waarde van de kosten heeft gevonden (wat maar het eerst optreedt). Initieel staat deze parameter op 30. Let op: het trainen van het netwerk kan even duren (je krijgt wel het iteratienummer te zien). Je hoeft voor deze opgave niets uit te programmeren. Bestudeer de werking van minimize en experimenteer met verschillende waarden voor de parameter maxiter om een goed beeld te krijgen van de werking van het geheel. Bekijk ook goed hoe de waarden van de matrices worden doorgegeven aan deze methode. Als het geheel is afgerond, worden de nieuwe kost en de nieuwe accuratesse van het netwerk getoond. Ook wordt er een plot gemaakt van de waarden van de matrix in de verborgen laag \u2013 zie de figuur hieronder. Met een beetje moeite en goede wil kun je zien dat deze matix een gevoeligheid heeft ontwikkeld voor horizontale en verticale lijnen en voor ronde vormen in de input. Met de getrainde waarde van het netwerk kunnen we nu voorspellingen doen over het getal dat door een afbeelding wordt gerepresenteerd. Maar de accuratesse is niet de enige metriek die voor de bepaling van hoe goed een netwerk is van belang is. Hierover gaan we het volgende week hebben, wanneer we de confusion matrix bespreken.","title":"4. trainen van het neurale netwerk"},{"location":"week3.html","text":"Opgaven week 3 \u00b6 Inleiding \u00b6 Deze week staat in het teken van TensorFlow en Keras . Tot nu toe hebben we de programmacode die bij de wiskunde hoorde zelf uitgeprogrammeerd, maar in de praktijk zul je dat niet heel vaak tegenkomen. Omdat de wiskunde behoorlijk complex kan worden, en feitelijk toch altijd min of meer hetzelfde is, zijn deze twee frameworks ontwikkeld om dergelijke implementatiedetails te abstraheren. Door deze (en vergelijkbare \u2013 er zijn er meer) frameworks te gebruiken, is het voor de ontwikkelaar mogelijk om zich te richten op de daadwerkelijke architectuur en optimalisatie-strategie\u00ebn.< In de eerste opgave gaan we werken met een standaard-dataset, die in TensorFlow is ingebakken, om een netwerk te trainen. De tweede opgave gaat op basis van dit netwerk een confusion matrix uitrekenen en tekenen. In de derde en laatste opgave moet je een hele architectuur from scratch uitdenken en uitwerken. De startcode en andere bestanden die bij deze opgave horen kun je hier downloaden . Net als de vorige twee weken is er een bestand exercise3.py , dat het bestand uitwerkingen.py gebruikt. Het is de bedoeling dat je dit laatste bestand afmaakt. Opgave 1: de fashion mnist \u00b6 In de eerste week hebben we gewerkt met de MNIST dataset, die handschriften van een paar duizend scholieren bevatte. Deze week gebruiken we een andere, vergelijkbare dataset, namelijk de fashion-mnist . Deze set bevat afbeeldingen van mode-items, zoals broeken, jurken en schoenen. De set bevat 60.000 trainingsplaatjes en 10.000 testplaatjes. Elk plaatje is een 28\u00d728 grayscale plaatje. Het bestand exercise3.py begint met het inladen van een aantal dependencies en laadt vervolgens de dataset in. Deze zit standaard in TensorFlow \u2013 bestudeer het script om hier een beeld van te krijgen. Het laden van de data kan de eerste keer even duren (uiteraard afhankelijk van de snelheid van je downstream ). Als de data geladen is, worden de dimensies van de verschillende data-sets uitgeprint. opgave 1a: het visualiseren en prepareren van de data \u00b6 Zoals altijd beginnen we met het visualiseren van de data. Maak de methode plotImage() in uitwerkingen af. Deze methode krijgt een array van 28\u00d728 als parameter mee en maakt gebruik van pyplotlib om hier een plaatje van te tekenen; ook wordt het label dat met het plaatje correspondeert aan de methode meegegeven. Zorg ervoor dat dit label onderaan het plaatje komt te staan. Het script exercise3.py roept deze methode aan met een willekeurige sample uit de dataset, zodat je eenvoudig kunt controleren of het plaatje correspondeert met het label. Net als de vorige weken kun je het tekenen van het plaatje vervolgens overslaan door de parameter skip aan het script mee te geven. opgave 1b: aanpassen van data \u00b6 Als je de data van de plaatjes bestudeert, zie je dat de getallen waaruit deze zijn opgemaakt liggen in de range van 0-255. Om deze goed door een neuraal netwerk te laten verwerken, is het van belang deze range om te zetten in getallen tussen de nul en de \u00e9\u00e9n. Implementeer de methode scaleData() , zodat de waarden van de getallen in de daaraan meegegeven matrix omgezet worden in de range 0-1. Zorg er daarbij voor, dat hier een willekeurige matrix aan kan worden meegegeven (dus ook \u00e9\u00e9n, waarbij de range van de oorsponkelijke waarden ligt tussen 0 en 1024). Maak gebruik van de numpy-methode amax om het hoogste getal in de meegegeven matrix te bepalen. Als je deze methode hebt ge\u00efmplementeerd, roept het script exercise3.py hem aan, zodat je kunt controleren of het klopt. Vervolgens wordt deze methode aangeroepen met train_images en test_images . opgave 1c: Het maken van het model \u00b6 Nu we de data hebben voorbereid, is het tijd om het model te maken. Tijdens de theorieles is ingegaan op de manier waarop je met Keras moet werken: dat moet je bij deze opgave toepassen. Maak in de methode createModel een netwerk met drie lagen: een input-laag die de plaatjes van 28\u00d728 omzet in 784 input-nodes; een tweede laag van 128 nodes die volledig verbonden is met de input-laag; en een derde laag met tien output-nodes. Geef aan de verborgen middelste laag een tf.nn.relu -activatie mee, en in de output-laag een tf.nn.softmax . Het kan zijn dat je wat deprecation-warnings krijgt bij het aanmaken van dit model (afhankelijk van de versie van TensorFlow die je gebruikt). Die kun je gevoegelijk negeren. Het model moet verder voorzien worden van een aantal parameters: De optimizer , die aangeeft hoe het model wordt ge\u00fcpdate op basis van de data en de loss-function . De loss-function , die aangeeft hoe de accuratesse van het model gedurende de trainingsronden wordt bepaald. De metrics , waarmee de training en de tests worden gemonitord. Geef deze parameters respectievelijk de waarden sparse_categorical_crossentropy , adam , en accuracy . Bestudeer de documentatie voor nadere specificaties hiervan. Retourneer het model vanuit de methode buildModel() . Wanneer deze opgave is afgerond, kun je het script exercise3.py opnieuw runnen. Hier wordt nu de methode fit op het model aangeroepen om het te trainen. In de volgende opgave gaan we vervolgens in op het bepalen van de kwaliteit van het getrainde netwerk. Opgave 2 \u2013 de confusion matrix \u00b6 In deze opgave gaan we op basis van de test-data bepalen hoe goed ons getrainde netwerk is. Zoals bekend is de accuratesse op zich niet voldoende om de presentatie van een classifier te bepalen: wanneer je gewoon zou gokken heb je bij tien mogelijke categorie\u00ebn al een score van tien procent, en als je zou gokken dat een sample iets niet is, is de accuratesse al negentig procent. Betere metrieken hiervoor worden gegeven door de confusion matrix , die tijdens de theorieles besproken is. Opgave 2a: bepalen van de confusion matrix \u00b6 Maak de methode confMatrix() af. Zoals je ziet krijgt deze methode twee parameters mee, namelijk de door het netwerk voorspelde waarden, en de daadwerkelijke waarden. Maak gebruik van de methode confusion_matrix in TensorFlow om deze matrix te bepalen. Retourneer de matrix. Als je hiermee klaar bent, wordt de methode door het script exercise3.py aangeroepen, met de voorspellingen van de test_images en actuele waarden van die test-data ( test_labels ). Het resultaat wordt vervolgens in een plot weergegeven; als het goed is, ziet het er ongeveer als volgt uit: Opgave 2b: TP, TN, FP, FN \u00b6 Om meer metrieken uit het getrainde model te halen, volstaat niet alleen het percentage van de samples dat correct is geclassificeerd; we moeten dan ook weten welk percentage terecht als niet van een bepaalde klasse is geclassificeerd, welke onterecht als wel van een bepaalde klasse, en welk percentage onterecht als wel van een bepaalde klasse: de zogenaamde true positives , true negatives , false positives en false negatives (zoals in het theoriecollege besproken is). De methode confEls() in uitwerkingen.py krijgt als parameter een numpy-array mee, die ook ten grondslag ligt aan de afbeelding hierboven. De regels van deze matrix corresponderen met de werkelijke waarde van het sample, de kolommen van deze matrix corresponderen met de voorspelling van het sample door het model. Hoewel er semantisch wel het \u00e9\u00e9n en ander op aan te merken is, defini\u00ebren we de hierboven beschreven metrieken als volgt: \\(tp_{i} = c_{ii}\\) \\(fp_{i} = \\sum_{l=1}^n c_{li} - tp_{i}\\) \\(fn_{i} = \\sum_{l=1}^n c_{il} - tp_{i}\\) \\(tn_{i} = \\sum_{l=1}^n \\sum_{k=1}^n c_{lk} -tp_{i} - fp_{i} - fn_{i}\\) Hierbij is \\(i\\) de categorie in kwestie (dus in dit specifieke geval loopt die van 1 - 10). Implementeer de methode confEls() , en retourneer een lijst met deze vier metrieken voor elk label \u2013 bestudeer het reeds gegeven deel van de implementatie om een beeld te krijgen van de exacte vorm van de return-waarde. Opgave 2c: precision en recall \u00b6 Implementeer nu de methode confData() , waarin je de data die je in de vorige opgave hebt gemaakt omzet in de onderstaande metrieken: \\(sensitivity (TPR) = \\frac{tp}{tp + fn}\\) \\(precision (PPV) = \\frac{tp}{tp + fp}\\) \\(specificity (TNR) = \\frac{tn}{tn + fp}\\) \\(fall-out (FPR) = \\frac{fp}{fp + tn}\\) Deze methode krijgt de lijst uit de vorige opgave mee: de totale \\(tp\\) is dan de som van alle \\(tp\\) 's van alle labels \u2013 en vergelijkbare berekeningen voor de total \\(tn\\) , \\(fp\\) en \\(fn\\) . Retourneer deze data als een dictionary. Als je deze beide methoden hebt ge\u00efmplementeerd, kun je het script exercise3.py nogmaals runnen; Hierdoor worden deze waarden afgedrukt. Zeg op basis van deze resultaten iets over de kwaliteit van het uitgeprogrammeerde netwerk. Opgave 3: Introductie Jupyter Notebook en SciKit Learn \u00b6 Zoals tijdens de theorieles besproken, maken we vanaf nu gebruik van Jupyter Notebook en van SciKit Learn. Download deze Notebook , start jupyter notebook en open de notebook. Volg de instructies die je daar ziet.","title":"Opgaven week 3"},{"location":"week3.html#opgaven-week-3","text":"","title":"Opgaven week 3"},{"location":"week3.html#inleiding","text":"Deze week staat in het teken van TensorFlow en Keras . Tot nu toe hebben we de programmacode die bij de wiskunde hoorde zelf uitgeprogrammeerd, maar in de praktijk zul je dat niet heel vaak tegenkomen. Omdat de wiskunde behoorlijk complex kan worden, en feitelijk toch altijd min of meer hetzelfde is, zijn deze twee frameworks ontwikkeld om dergelijke implementatiedetails te abstraheren. Door deze (en vergelijkbare \u2013 er zijn er meer) frameworks te gebruiken, is het voor de ontwikkelaar mogelijk om zich te richten op de daadwerkelijke architectuur en optimalisatie-strategie\u00ebn.< In de eerste opgave gaan we werken met een standaard-dataset, die in TensorFlow is ingebakken, om een netwerk te trainen. De tweede opgave gaat op basis van dit netwerk een confusion matrix uitrekenen en tekenen. In de derde en laatste opgave moet je een hele architectuur from scratch uitdenken en uitwerken. De startcode en andere bestanden die bij deze opgave horen kun je hier downloaden . Net als de vorige twee weken is er een bestand exercise3.py , dat het bestand uitwerkingen.py gebruikt. Het is de bedoeling dat je dit laatste bestand afmaakt.","title":"Inleiding"},{"location":"week3.html#opgave-1-de-fashion-mnist","text":"In de eerste week hebben we gewerkt met de MNIST dataset, die handschriften van een paar duizend scholieren bevatte. Deze week gebruiken we een andere, vergelijkbare dataset, namelijk de fashion-mnist . Deze set bevat afbeeldingen van mode-items, zoals broeken, jurken en schoenen. De set bevat 60.000 trainingsplaatjes en 10.000 testplaatjes. Elk plaatje is een 28\u00d728 grayscale plaatje. Het bestand exercise3.py begint met het inladen van een aantal dependencies en laadt vervolgens de dataset in. Deze zit standaard in TensorFlow \u2013 bestudeer het script om hier een beeld van te krijgen. Het laden van de data kan de eerste keer even duren (uiteraard afhankelijk van de snelheid van je downstream ). Als de data geladen is, worden de dimensies van de verschillende data-sets uitgeprint.","title":"Opgave 1: de fashion mnist"},{"location":"week3.html#opgave-1a-het-visualiseren-en-prepareren-van-de-data","text":"Zoals altijd beginnen we met het visualiseren van de data. Maak de methode plotImage() in uitwerkingen af. Deze methode krijgt een array van 28\u00d728 als parameter mee en maakt gebruik van pyplotlib om hier een plaatje van te tekenen; ook wordt het label dat met het plaatje correspondeert aan de methode meegegeven. Zorg ervoor dat dit label onderaan het plaatje komt te staan. Het script exercise3.py roept deze methode aan met een willekeurige sample uit de dataset, zodat je eenvoudig kunt controleren of het plaatje correspondeert met het label. Net als de vorige weken kun je het tekenen van het plaatje vervolgens overslaan door de parameter skip aan het script mee te geven.","title":"opgave 1a: het visualiseren en prepareren van de data"},{"location":"week3.html#opgave-1b-aanpassen-van-data","text":"Als je de data van de plaatjes bestudeert, zie je dat de getallen waaruit deze zijn opgemaakt liggen in de range van 0-255. Om deze goed door een neuraal netwerk te laten verwerken, is het van belang deze range om te zetten in getallen tussen de nul en de \u00e9\u00e9n. Implementeer de methode scaleData() , zodat de waarden van de getallen in de daaraan meegegeven matrix omgezet worden in de range 0-1. Zorg er daarbij voor, dat hier een willekeurige matrix aan kan worden meegegeven (dus ook \u00e9\u00e9n, waarbij de range van de oorsponkelijke waarden ligt tussen 0 en 1024). Maak gebruik van de numpy-methode amax om het hoogste getal in de meegegeven matrix te bepalen. Als je deze methode hebt ge\u00efmplementeerd, roept het script exercise3.py hem aan, zodat je kunt controleren of het klopt. Vervolgens wordt deze methode aangeroepen met train_images en test_images .","title":"opgave 1b: aanpassen van data"},{"location":"week3.html#opgave-1c-het-maken-van-het-model","text":"Nu we de data hebben voorbereid, is het tijd om het model te maken. Tijdens de theorieles is ingegaan op de manier waarop je met Keras moet werken: dat moet je bij deze opgave toepassen. Maak in de methode createModel een netwerk met drie lagen: een input-laag die de plaatjes van 28\u00d728 omzet in 784 input-nodes; een tweede laag van 128 nodes die volledig verbonden is met de input-laag; en een derde laag met tien output-nodes. Geef aan de verborgen middelste laag een tf.nn.relu -activatie mee, en in de output-laag een tf.nn.softmax . Het kan zijn dat je wat deprecation-warnings krijgt bij het aanmaken van dit model (afhankelijk van de versie van TensorFlow die je gebruikt). Die kun je gevoegelijk negeren. Het model moet verder voorzien worden van een aantal parameters: De optimizer , die aangeeft hoe het model wordt ge\u00fcpdate op basis van de data en de loss-function . De loss-function , die aangeeft hoe de accuratesse van het model gedurende de trainingsronden wordt bepaald. De metrics , waarmee de training en de tests worden gemonitord. Geef deze parameters respectievelijk de waarden sparse_categorical_crossentropy , adam , en accuracy . Bestudeer de documentatie voor nadere specificaties hiervan. Retourneer het model vanuit de methode buildModel() . Wanneer deze opgave is afgerond, kun je het script exercise3.py opnieuw runnen. Hier wordt nu de methode fit op het model aangeroepen om het te trainen. In de volgende opgave gaan we vervolgens in op het bepalen van de kwaliteit van het getrainde netwerk.","title":"opgave 1c: Het maken van het model"},{"location":"week3.html#opgave-2-de-confusion-matrix","text":"In deze opgave gaan we op basis van de test-data bepalen hoe goed ons getrainde netwerk is. Zoals bekend is de accuratesse op zich niet voldoende om de presentatie van een classifier te bepalen: wanneer je gewoon zou gokken heb je bij tien mogelijke categorie\u00ebn al een score van tien procent, en als je zou gokken dat een sample iets niet is, is de accuratesse al negentig procent. Betere metrieken hiervoor worden gegeven door de confusion matrix , die tijdens de theorieles besproken is.","title":"Opgave 2 \u2013 de confusion matrix"},{"location":"week3.html#opgave-2a-bepalen-van-de-confusion-matrix","text":"Maak de methode confMatrix() af. Zoals je ziet krijgt deze methode twee parameters mee, namelijk de door het netwerk voorspelde waarden, en de daadwerkelijke waarden. Maak gebruik van de methode confusion_matrix in TensorFlow om deze matrix te bepalen. Retourneer de matrix. Als je hiermee klaar bent, wordt de methode door het script exercise3.py aangeroepen, met de voorspellingen van de test_images en actuele waarden van die test-data ( test_labels ). Het resultaat wordt vervolgens in een plot weergegeven; als het goed is, ziet het er ongeveer als volgt uit:","title":"Opgave 2a: bepalen van de confusion matrix"},{"location":"week3.html#opgave-2b-tp-tn-fp-fn","text":"Om meer metrieken uit het getrainde model te halen, volstaat niet alleen het percentage van de samples dat correct is geclassificeerd; we moeten dan ook weten welk percentage terecht als niet van een bepaalde klasse is geclassificeerd, welke onterecht als wel van een bepaalde klasse, en welk percentage onterecht als wel van een bepaalde klasse: de zogenaamde true positives , true negatives , false positives en false negatives (zoals in het theoriecollege besproken is). De methode confEls() in uitwerkingen.py krijgt als parameter een numpy-array mee, die ook ten grondslag ligt aan de afbeelding hierboven. De regels van deze matrix corresponderen met de werkelijke waarde van het sample, de kolommen van deze matrix corresponderen met de voorspelling van het sample door het model. Hoewel er semantisch wel het \u00e9\u00e9n en ander op aan te merken is, defini\u00ebren we de hierboven beschreven metrieken als volgt: \\(tp_{i} = c_{ii}\\) \\(fp_{i} = \\sum_{l=1}^n c_{li} - tp_{i}\\) \\(fn_{i} = \\sum_{l=1}^n c_{il} - tp_{i}\\) \\(tn_{i} = \\sum_{l=1}^n \\sum_{k=1}^n c_{lk} -tp_{i} - fp_{i} - fn_{i}\\) Hierbij is \\(i\\) de categorie in kwestie (dus in dit specifieke geval loopt die van 1 - 10). Implementeer de methode confEls() , en retourneer een lijst met deze vier metrieken voor elk label \u2013 bestudeer het reeds gegeven deel van de implementatie om een beeld te krijgen van de exacte vorm van de return-waarde.","title":"Opgave 2b: TP, TN, FP, FN"},{"location":"week3.html#opgave-2c-precision-en-recall","text":"Implementeer nu de methode confData() , waarin je de data die je in de vorige opgave hebt gemaakt omzet in de onderstaande metrieken: \\(sensitivity (TPR) = \\frac{tp}{tp + fn}\\) \\(precision (PPV) = \\frac{tp}{tp + fp}\\) \\(specificity (TNR) = \\frac{tn}{tn + fp}\\) \\(fall-out (FPR) = \\frac{fp}{fp + tn}\\) Deze methode krijgt de lijst uit de vorige opgave mee: de totale \\(tp\\) is dan de som van alle \\(tp\\) 's van alle labels \u2013 en vergelijkbare berekeningen voor de total \\(tn\\) , \\(fp\\) en \\(fn\\) . Retourneer deze data als een dictionary. Als je deze beide methoden hebt ge\u00efmplementeerd, kun je het script exercise3.py nogmaals runnen; Hierdoor worden deze waarden afgedrukt. Zeg op basis van deze resultaten iets over de kwaliteit van het uitgeprogrammeerde netwerk.","title":"Opgave 2c: precision en recall"},{"location":"week3.html#opgave-3-introductie-jupyter-notebook-en-scikit-learn","text":"Zoals tijdens de theorieles besproken, maken we vanaf nu gebruik van Jupyter Notebook en van SciKit Learn. Download deze Notebook , start jupyter notebook en open de notebook. Volg de instructies die je daar ziet.","title":"Opgave 3: Introductie Jupyter Notebook en SciKit Learn"},{"location":"week4.html","text":"Opgaven week 4 \u00b6 Model-evaluatie \u00b6 Download de notebook over model-evaluatie en voer de stappen uit.","title":"Opgaven week 4"},{"location":"week4.html#opgaven-week-4","text":"","title":"Opgaven week 4"},{"location":"week4.html#model-evaluatie","text":"Download de notebook over model-evaluatie en voer de stappen uit.","title":"Model-evaluatie"},{"location":"files/Introductie%20Pandas.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Introductie Pandas \u00b6","title":"Introductie Pandas"},{"location":"files/Introductie%20Pandas.html#introductie-pandas","text":"","title":"Introductie Pandas"},{"location":"files/Opdracht%20model-evaluatie.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Opdracht model-evaluatie \u00b6 download de open notebook hier . from sklearn.datasets import load_breast_cancer import numpy as np Opdracht 1 \u00b6 Laad de borstkanker-dataset en maak gebruik van DESCR om een beeld te krijgen van de gegevens die in deze dataset zijn opgeslagen. zorg ervoor dat je de features in een variabele X krijgt en de targets in de variabele y (dit kan op minimaal twee manieren). Je hoeft voor deze opgave geen EDA te maken of de data helemaal op te schonen (mag natuurlijk wel). Je hoeft het niet allemaal in \u00e9\u00e9n cel te doen; voel je vrij om meer cellen aan te maken wanneer je dat wilt. # YOUR CODE HERE Maak een Support Vector Classifier met de standaard-waarden voor alle parameters. Geef dit model mee aan plot_learning_curve die in helpers.py te vinden is. Behalve dit model verwacht die methode eveneens een titel, de X en de y . De volledige signature van die methode staat hieronder; bestudeer eventueel de code om de volledige implementatie te zien. plot_learning_curve( estimator, title, X, y, axes=None, ylim=None, cv=None, n_jobs=None, scoring=\"accuracy\", train_sizes=np.linspace(0.1, 1.0, 5), ) from sklearn.svm import SVC from helpers import plot_learning_curve # YOUR CODE HERE Als het goed is, heb je nu hierboven drie grafieken staan. Bedenk op basis van deze visualisatie hoe goed of hoe slecht je vindt dat je classifier werkt. Experimenteer vervolgens met verschillende waarden voor de parameters van die SVC : verander de kernel en verhoog (als je kernel poly is) de degree . Welke verschillen zie je in de visualisatis? Kun je op basis hiervan een voorstel doen voor de beste waarden voor die parameters? Maak gebruik van train_test_split om de data op te splitsen in tachtig procent trainingsdata en twintig procent testdata. Train een SVC op basis van de beste parameters die je hierboven hebt ge\u00efdentificeerd. Maak vervolgens een confusion matrix en een classificatie-raport op basis van de testdata met dit model. Maak tenslotte een ROC-curve van dit getrainde model. Geef op basis hiervan een analyse van de kwaliteit van het model en een advies over hoe het model eventueel te verbeteren zou zijn. from sklearn.model_selection import train_test_split # YOUR CODE HERE from sklearn.metrics import confusion_matrix from sklearn.metrics import ConfusionMatrixDisplay , RocCurveDisplay # YOUR CODE HERE # Plot een confusion-matrix. # Maak gebruik van de klasse ConfusionMatrixDisplay die hierboven is ge\u00efmporteerd # YOUR CODE HERE # Plot een ROC-curve. # Maak gebruik van de klasse RocCurveDisplay die hierboven is ge\u00efmporteerd # YOUR CODE HERE Opdracht 2 \u00b6 Maak en train nu verschillende andere typen classifiers (een aantal is hieronder gegeven, maar voel je vrij om een andere set te gebruiken). Let op: alle classifiers in sklearn implementeren dezelfde interface: maak hiervan gebruik in je realisatie. In de cel hieronder wordt een DataFrame result_table gedefinieerd. Het is de bedoeling dat je van alle classifiers die je gebruikt en traint de fpr , de tpr en de auc in dit DataFrame opslaat. Je kunt hiervoor gebruik maken van de sklearn-methoden roc_curve en roc_auc_score . import pandas as pd import numpy as np % matplotlib inline from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import roc_curve , roc_auc_score # DataFrame om de gevonden metrieken per classifier in op te slaan. result_table = pd . DataFrame ( columns = [ 'classifiers' , 'fpr' , 'tpr' , 'auc' ]) # YOUR CODE HERE In de cel hieronder wordt de variabele result_table gebruikt om de verschillende ROC's in \u00e9\u00e9n figuur te plotten. Je hoeft hiervoor niks te programmeren; als je de cel runt krijgt je als het goed is direct de juiste visualisatie. Kun je op basis van deze visualisatie een uitspraak doen over welk model de beste performance heeft voor deze dataset? import matplotlib.pyplot as plt fig = plt . figure ( figsize = ( 8 , 6 )) for i in result_table . index : plt . plot ( result_table . loc [ i ][ 'fpr' ], result_table . loc [ i ][ 'tpr' ], label = f \" { i } , AUC= { result_table . loc [ i ][ 'auc' ] : .3f } \" ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], color = 'orange' , linestyle = '--' ) plt . xticks ( np . arange ( 0.0 , 1.1 , step = 0.1 )) plt . xlabel ( \"False Positive Rate\" , fontsize = 15 ) plt . yticks ( np . arange ( 0.0 , 1.1 , step = 0.1 )) plt . ylabel ( \"True Positive Rate\" , fontsize = 15 ) plt . title ( 'ROC Curve Analysis' , fontweight = 'bold' , fontsize = 15 ) plt . legend ( prop = { 'size' : 13 }, loc = 'lower right' ) plt . show ()","title":"Opdracht model evaluatie"},{"location":"files/Opdracht%20model-evaluatie.html#opdracht-model-evaluatie","text":"download de open notebook hier . from sklearn.datasets import load_breast_cancer import numpy as np","title":"Opdracht model-evaluatie"},{"location":"files/Opdracht%20model-evaluatie.html#opdracht-1","text":"Laad de borstkanker-dataset en maak gebruik van DESCR om een beeld te krijgen van de gegevens die in deze dataset zijn opgeslagen. zorg ervoor dat je de features in een variabele X krijgt en de targets in de variabele y (dit kan op minimaal twee manieren). Je hoeft voor deze opgave geen EDA te maken of de data helemaal op te schonen (mag natuurlijk wel). Je hoeft het niet allemaal in \u00e9\u00e9n cel te doen; voel je vrij om meer cellen aan te maken wanneer je dat wilt. # YOUR CODE HERE Maak een Support Vector Classifier met de standaard-waarden voor alle parameters. Geef dit model mee aan plot_learning_curve die in helpers.py te vinden is. Behalve dit model verwacht die methode eveneens een titel, de X en de y . De volledige signature van die methode staat hieronder; bestudeer eventueel de code om de volledige implementatie te zien. plot_learning_curve( estimator, title, X, y, axes=None, ylim=None, cv=None, n_jobs=None, scoring=\"accuracy\", train_sizes=np.linspace(0.1, 1.0, 5), ) from sklearn.svm import SVC from helpers import plot_learning_curve # YOUR CODE HERE Als het goed is, heb je nu hierboven drie grafieken staan. Bedenk op basis van deze visualisatie hoe goed of hoe slecht je vindt dat je classifier werkt. Experimenteer vervolgens met verschillende waarden voor de parameters van die SVC : verander de kernel en verhoog (als je kernel poly is) de degree . Welke verschillen zie je in de visualisatis? Kun je op basis hiervan een voorstel doen voor de beste waarden voor die parameters? Maak gebruik van train_test_split om de data op te splitsen in tachtig procent trainingsdata en twintig procent testdata. Train een SVC op basis van de beste parameters die je hierboven hebt ge\u00efdentificeerd. Maak vervolgens een confusion matrix en een classificatie-raport op basis van de testdata met dit model. Maak tenslotte een ROC-curve van dit getrainde model. Geef op basis hiervan een analyse van de kwaliteit van het model en een advies over hoe het model eventueel te verbeteren zou zijn. from sklearn.model_selection import train_test_split # YOUR CODE HERE from sklearn.metrics import confusion_matrix from sklearn.metrics import ConfusionMatrixDisplay , RocCurveDisplay # YOUR CODE HERE # Plot een confusion-matrix. # Maak gebruik van de klasse ConfusionMatrixDisplay die hierboven is ge\u00efmporteerd # YOUR CODE HERE # Plot een ROC-curve. # Maak gebruik van de klasse RocCurveDisplay die hierboven is ge\u00efmporteerd # YOUR CODE HERE","title":"Opdracht 1"},{"location":"files/Opdracht%20model-evaluatie.html#opdracht-2","text":"Maak en train nu verschillende andere typen classifiers (een aantal is hieronder gegeven, maar voel je vrij om een andere set te gebruiken). Let op: alle classifiers in sklearn implementeren dezelfde interface: maak hiervan gebruik in je realisatie. In de cel hieronder wordt een DataFrame result_table gedefinieerd. Het is de bedoeling dat je van alle classifiers die je gebruikt en traint de fpr , de tpr en de auc in dit DataFrame opslaat. Je kunt hiervoor gebruik maken van de sklearn-methoden roc_curve en roc_auc_score . import pandas as pd import numpy as np % matplotlib inline from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import roc_curve , roc_auc_score # DataFrame om de gevonden metrieken per classifier in op te slaan. result_table = pd . DataFrame ( columns = [ 'classifiers' , 'fpr' , 'tpr' , 'auc' ]) # YOUR CODE HERE In de cel hieronder wordt de variabele result_table gebruikt om de verschillende ROC's in \u00e9\u00e9n figuur te plotten. Je hoeft hiervoor niks te programmeren; als je de cel runt krijgt je als het goed is direct de juiste visualisatie. Kun je op basis van deze visualisatie een uitspraak doen over welk model de beste performance heeft voor deze dataset? import matplotlib.pyplot as plt fig = plt . figure ( figsize = ( 8 , 6 )) for i in result_table . index : plt . plot ( result_table . loc [ i ][ 'fpr' ], result_table . loc [ i ][ 'tpr' ], label = f \" { i } , AUC= { result_table . loc [ i ][ 'auc' ] : .3f } \" ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], color = 'orange' , linestyle = '--' ) plt . xticks ( np . arange ( 0.0 , 1.1 , step = 0.1 )) plt . xlabel ( \"False Positive Rate\" , fontsize = 15 ) plt . yticks ( np . arange ( 0.0 , 1.1 , step = 0.1 )) plt . ylabel ( \"True Positive Rate\" , fontsize = 15 ) plt . title ( 'ROC Curve Analysis' , fontweight = 'bold' , fontsize = 15 ) plt . legend ( prop = { 'size' : 13 }, loc = 'lower right' ) plt . show ()","title":"Opdracht 2"},{"location":"files/intro%20notebook%20en%20sklearn.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Download de jupyter notebook hier . Introductie Jupyter Notebook en Scikit-learn \u00b6 Zoals tijdens het theoriecollege is toegelicht, maken we vanaf deze week gebruik van Jupyter Notebooks, een feitelijke standaard voor het rapid prototyping van machine learning projecten. Het grote voordeel van notebooks is dat je de documentatie (in markdown ) direct tussen je runbare code hebt staan. Hoewel oorspronkelijk ontwikkeld voor Python zijn er inmiddels voor de meeste talen kernels ontwikkeld, zodat je ook Java, Go of PHP in notebooks kunt schrijven. Een tweede stap die we nu gaan zetten in gebruik maken van een bibliotheek om het zware werk voor ons over te nemen: skikit learn . Tot nu toe schreven we alle code zelf, maar in het echt maak je gebruik van deze bibliotheek: die is sneller en makkelijker en stelt je in staat om je te richten op het maken en beoordelen van modellen in plaats van het goedlaten werken van feitelijk vrij triviale programmacode. E\u00e9n van de voordelen van sklearn is dat de meest gebruikte datasets standaard in deze bibliotheek zitten. Veel van de voorbeelden waar we de voorbije weken mee gewerkt hebben, zul je hierin terugvinden. In deze opgave maken we gebruik van de California Housing dataset . Run de volgende cel om de methode te importeren die deze dataset kan laden. Bestudeer de documentatie om te weten te komen wat er in deze dataset is opgeslagen en hoe je vervolgens de data daadwerkelijk laadt. # https://stackoverflow.com/a/49174340 # Haal de onderstaande regel uit het commentaar als je SSL-errors krijgt: # ssl._create_default_https_context = ssl._create_unverified_context from sklearn.datasets import fetch_california_housing import matplotlib.pyplot as plt import numpy as np Opdracht 1: data laden en inspecteren \u00b6 Gebruik de onderstaande cel om de methode fetch_california_housing aan te roepen. Mocht je bij het laden SSL-errors krijgen, probeer dan de eerste regel in de bovenstaande cel uit het commentaar te halen en run die cel nogmaals. Gebruik feature_names om de namen van de eigenschappen van de dataset te weten te komen. Zorg ervoor dat je de data van het resultaat in een variabele X zet, en de target in een variabele y . # YOUR CODE HERE Zoals altijd maken we ook een paar visualisaties van de data om een beeld te krijgen van wat er zoal in zit. We beginnen met een scatter-plot; alleen dit keer plotten we niet de \\(y\\) -vector tegen een eigenschap uit de \\(X\\) -matrix; omdat we weten dat we te maken hebben met geografische data, is het leuker om de lengte- en breedtegraden tegenover elkaar te plotten. Maar gebruik van matplotlib.pyplot.scatter om deze twee gegevens ( Longitude en Latitude , respectievelijk) te plotten. Als je het goed hebt gedaan, kun je in de resulterende plot de kustlijn van Californi\u00eb herkennen. #YOUR CODE HERE Zoals je in de documentatie hebt gelezen, is de target-value de gemiddelde waarde van de huizen in die omgeving, uitgedrukt in honderdduizend dollar. Natuurlijk moeten we wat statistieken uit deze target-vector halen. Vul onderstaande cel aan, zodat de juiste waarden worden afgedrukt. Maar vervolgens gebruik van pyplot.hist om een histogram van deze data te plotten. Beargumenteer op basis van de statistische gegevens in hoeveel bins je deze histogram moet onderverdelen. import statistics as st # YOUR CODE HERE # vervang '0' door de juiste code min_value = 0 max_value = 0 stdev = 0 gemiddelde = 0 print ( '==== DATA UIT DE TARGET-VECTOR ====' ) print ( f 'Gemiddelde: { gemiddelde : >10.2f } ' ) print ( f 'Minimum: { min_value : >10.2f } ' ) print ( f 'Maximum: { max_value : >10.2f } ' ) print ( f 'StdDev: { stdev : >10.2f } ' ) Een belangrijke stap om een beeld te krijgen van de data in de set is door gebruik te maken van een histogram . E\u00e9n van de belangrijke vragen daarbij is in hoeveel bins je de data moet verdelen. Daarvoor zijn grofweg twee methoden: Sturge's Rule en Freedman-Diaconis rule . Bestudeer deze blog hierover maak beide histogrammen. Let op dat het aantal bins een geheel getal moet zijn. Als het goed is, kom je in het eerste geval op 16 bins en in het tweede geval op 46. Welke van beide histogrammen vind je beter en waarom? # histogram met Sturge's Rule m , n = X . shape # YOUR CODE HERE # histogram met Freedman-Diaconis rule m , n = X . shape # YOUR CODE HERE Opdracht 2: Lineaire regressie \u00b6 Nu gaan we de features van deze dataset gebruiken om een voorspelling te doen van de waarde van een huis. In week 1 hebben we de wiskunde daarvan helemaal uitgeprogrammeerd; nu maken we gebruik van 'sklearn.linear_model.linear_regression' . Verdeel de data in 20% testdata en 80% trainingsdata. Maak hiervoor gebruik van train_test_split . Laad de data opnieuw in met de parameter return_X_y op True , zodat je direct de features en de corresponderende targets hebt. Waarom is deze split ook al weer nodig? Gebruik vervolgens de methode fit om het model te trainen. from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression #YOUR CODE HERE Gebruik nu de methode predict om op basis van de test-data een uitspraak te doen over hoe goed het model presteert. Gebruik hiervoor de methode mean_square_error uit sklearn.metrics . Hoe vind je dat het model presteert? Wat zou je kunnen doen om het model te verbeteren? from sklearn.metrics import mean_squared_error #YOUR CODE HERE Bestudeer tenslotte met behulp van het coef_ -attribuut van het getrainde model om te weten te komen wat de formule is die het model gebruikt. Welke features zijn het belangrijkst en welke minder? #YOUR CODE HERE","title":"Intro notebook en sklearn"},{"location":"files/intro%20notebook%20en%20sklearn.html#introductie-jupyter-notebook-en-scikit-learn","text":"Zoals tijdens het theoriecollege is toegelicht, maken we vanaf deze week gebruik van Jupyter Notebooks, een feitelijke standaard voor het rapid prototyping van machine learning projecten. Het grote voordeel van notebooks is dat je de documentatie (in markdown ) direct tussen je runbare code hebt staan. Hoewel oorspronkelijk ontwikkeld voor Python zijn er inmiddels voor de meeste talen kernels ontwikkeld, zodat je ook Java, Go of PHP in notebooks kunt schrijven. Een tweede stap die we nu gaan zetten in gebruik maken van een bibliotheek om het zware werk voor ons over te nemen: skikit learn . Tot nu toe schreven we alle code zelf, maar in het echt maak je gebruik van deze bibliotheek: die is sneller en makkelijker en stelt je in staat om je te richten op het maken en beoordelen van modellen in plaats van het goedlaten werken van feitelijk vrij triviale programmacode. E\u00e9n van de voordelen van sklearn is dat de meest gebruikte datasets standaard in deze bibliotheek zitten. Veel van de voorbeelden waar we de voorbije weken mee gewerkt hebben, zul je hierin terugvinden. In deze opgave maken we gebruik van de California Housing dataset . Run de volgende cel om de methode te importeren die deze dataset kan laden. Bestudeer de documentatie om te weten te komen wat er in deze dataset is opgeslagen en hoe je vervolgens de data daadwerkelijk laadt. # https://stackoverflow.com/a/49174340 # Haal de onderstaande regel uit het commentaar als je SSL-errors krijgt: # ssl._create_default_https_context = ssl._create_unverified_context from sklearn.datasets import fetch_california_housing import matplotlib.pyplot as plt import numpy as np","title":"Introductie Jupyter Notebook en Scikit-learn"},{"location":"files/intro%20notebook%20en%20sklearn.html#opdracht-1-data-laden-en-inspecteren","text":"Gebruik de onderstaande cel om de methode fetch_california_housing aan te roepen. Mocht je bij het laden SSL-errors krijgen, probeer dan de eerste regel in de bovenstaande cel uit het commentaar te halen en run die cel nogmaals. Gebruik feature_names om de namen van de eigenschappen van de dataset te weten te komen. Zorg ervoor dat je de data van het resultaat in een variabele X zet, en de target in een variabele y . # YOUR CODE HERE Zoals altijd maken we ook een paar visualisaties van de data om een beeld te krijgen van wat er zoal in zit. We beginnen met een scatter-plot; alleen dit keer plotten we niet de \\(y\\) -vector tegen een eigenschap uit de \\(X\\) -matrix; omdat we weten dat we te maken hebben met geografische data, is het leuker om de lengte- en breedtegraden tegenover elkaar te plotten. Maar gebruik van matplotlib.pyplot.scatter om deze twee gegevens ( Longitude en Latitude , respectievelijk) te plotten. Als je het goed hebt gedaan, kun je in de resulterende plot de kustlijn van Californi\u00eb herkennen. #YOUR CODE HERE Zoals je in de documentatie hebt gelezen, is de target-value de gemiddelde waarde van de huizen in die omgeving, uitgedrukt in honderdduizend dollar. Natuurlijk moeten we wat statistieken uit deze target-vector halen. Vul onderstaande cel aan, zodat de juiste waarden worden afgedrukt. Maar vervolgens gebruik van pyplot.hist om een histogram van deze data te plotten. Beargumenteer op basis van de statistische gegevens in hoeveel bins je deze histogram moet onderverdelen. import statistics as st # YOUR CODE HERE # vervang '0' door de juiste code min_value = 0 max_value = 0 stdev = 0 gemiddelde = 0 print ( '==== DATA UIT DE TARGET-VECTOR ====' ) print ( f 'Gemiddelde: { gemiddelde : >10.2f } ' ) print ( f 'Minimum: { min_value : >10.2f } ' ) print ( f 'Maximum: { max_value : >10.2f } ' ) print ( f 'StdDev: { stdev : >10.2f } ' ) Een belangrijke stap om een beeld te krijgen van de data in de set is door gebruik te maken van een histogram . E\u00e9n van de belangrijke vragen daarbij is in hoeveel bins je de data moet verdelen. Daarvoor zijn grofweg twee methoden: Sturge's Rule en Freedman-Diaconis rule . Bestudeer deze blog hierover maak beide histogrammen. Let op dat het aantal bins een geheel getal moet zijn. Als het goed is, kom je in het eerste geval op 16 bins en in het tweede geval op 46. Welke van beide histogrammen vind je beter en waarom? # histogram met Sturge's Rule m , n = X . shape # YOUR CODE HERE # histogram met Freedman-Diaconis rule m , n = X . shape # YOUR CODE HERE","title":"Opdracht 1: data laden en inspecteren"},{"location":"files/intro%20notebook%20en%20sklearn.html#opdracht-2-lineaire-regressie","text":"Nu gaan we de features van deze dataset gebruiken om een voorspelling te doen van de waarde van een huis. In week 1 hebben we de wiskunde daarvan helemaal uitgeprogrammeerd; nu maken we gebruik van 'sklearn.linear_model.linear_regression' . Verdeel de data in 20% testdata en 80% trainingsdata. Maak hiervoor gebruik van train_test_split . Laad de data opnieuw in met de parameter return_X_y op True , zodat je direct de features en de corresponderende targets hebt. Waarom is deze split ook al weer nodig? Gebruik vervolgens de methode fit om het model te trainen. from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression #YOUR CODE HERE Gebruik nu de methode predict om op basis van de test-data een uitspraak te doen over hoe goed het model presteert. Gebruik hiervoor de methode mean_square_error uit sklearn.metrics . Hoe vind je dat het model presteert? Wat zou je kunnen doen om het model te verbeteren? from sklearn.metrics import mean_squared_error #YOUR CODE HERE Bestudeer tenslotte met behulp van het coef_ -attribuut van het getrainde model om te weten te komen wat de formule is die het model gebruikt. Welke features zijn het belangrijkst en welke minder? #YOUR CODE HERE","title":"Opdracht 2: Lineaire regressie"}]}